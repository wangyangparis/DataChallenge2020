# Data-Challenge-2020

Xgboost use a **second ordre Taylor approximation**,light gbm also request the gradient and the hessien in its cost function, very alike to Xgboost. For binary classification, we use a log loss:
$$ğ¿=âˆ’ğ‘¦lnğ‘âˆ’ğ›½(1âˆ’ğ‘¦)ln(1âˆ’ğ‘)$$ $p$ as the probability estimated by sigmoid function. **$ğ›½$ is the multiplier factor to increase the weight of FP loss**.

In order to **penalise False Positive**, I put a penalty Beta on the FP, I can calculate the gradient as: 

$$grad =\frac{âˆ‚ğ¿}{âˆ‚ğ‘¥}=\frac{âˆ‚ğ¿}{âˆ‚ğ‘}\frac{âˆ‚ğ‘}{âˆ‚ğ‘¥}=ğ‘(ğ›½+ğ‘¦âˆ’ğ›½ğ‘¦)âˆ’ğ‘¦ $$,

and hessien as:

$$ hess =âˆ‚2ğ¿âˆ‚ğ‘¥2=ğ‘(1âˆ’ğ‘)(ğ›½+ğ‘¦âˆ’ğ›½ğ‘¦) $$

 **Customizing the training loss** in LightGBM requires defining a function that takes in two arrays, the targets and their predictions. In turn, the function should return two arrays of the gradient and hessian of each observation. As noted above, we need to use calculus to derive gradient and hessian and then implement it in Python.
